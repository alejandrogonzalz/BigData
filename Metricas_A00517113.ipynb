{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsZKKvxwYF3o"
   },
   "source": [
    "# Actividad 4 | Métricas de calidad de resultados\n",
    "Identificar métricas para la medición de la calidad de resultados derivados de la aplicación de modelos de aprendizaje, ya sea supervisado o no supervisado, orientado al procesamiento de grandes volúmenes de datos, que permitan la selección de los modelos que mejor se ajusten a la tarea de aprendizaje a resolver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjmB97B-X-hU"
   },
   "source": "## Alejandro González Almazán - A00517113\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0n2G2l_Yaze"
   },
   "source": "--------------------------------------------------------------------------------"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar8PqEFjYfVA"
   },
   "source": "# Impotacion de Librerias"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rFrmYAiPYdd9",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:02.569411Z",
     "start_time": "2025-06-09T02:20:02.133065Z"
    }
   },
   "source": [
    "# PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\"\"\"\n",
    "# omitir para ejecutar de forma local\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\"\"\"\n",
    "#Librerias de codigo\n",
    "import kagglehub\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXLHPUosZE5_"
   },
   "source": "### Inicializar entorno PySpark"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "GCAP0o7kcogx",
    "outputId": "1ed9ae58-97d4-422d-b102-c39be4f404db",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:12.357232Z",
     "start_time": "2025-06-09T02:20:02.652399Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analisis_Steam\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b569715be0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Alex:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analisis_Steam</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm2nuBbCZct4"
   },
   "source": "### Lectura de datos"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nTiLMXpKZ3z9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "750a6611-b09f-4fb9-91a3-edf92c244175",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.376742Z",
     "start_time": "2025-06-09T02:20:13.139045Z"
    }
   },
   "source": "file_path = kagglehub.dataset_download(\"najzeko/steam-reviews-2021\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.632899Z",
     "start_time": "2025-06-09T02:20:13.629182Z"
    }
   },
   "cell_type": "code",
   "source": "print(file_path)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\najzeko\\steam-reviews-2021\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlaqOdpzrRV5",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.719225Z",
     "start_time": "2025-06-09T02:20:13.713104Z"
    }
   },
   "source": "print(f\"{file_path}/steam_reviews.csv\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\najzeko\\steam-reviews-2021\\versions\\1/steam_reviews.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BGv7gw7jZdgb",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:21:24.728261Z",
     "start_time": "2025-06-09T02:20:13.814533Z"
    }
   },
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"sep\", \",\").option(\"escape\", \"\\\"\").csv(f\"{file_path}/steam_reviews.csv\")\n",
    "\n",
    "new_columns = [col_name.replace(\".\", \"_\") for col_name in df.columns]\n",
    "df = df.toDF(*new_columns)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F36rcs_erRWC"
   },
   "source": [
    "## 1. Construcción de la muestra M (Particiones Mi) con PySpark\n",
    "Para construir una muestra M representativa de la población P (dataset de reseñas de Steam), se generarán particiones Mi basadas en variables de caracterización clave. El objetivo es asegurar que cada Mi mantenga proporcionalidad respecto a la distribución original, evitando sesgos.\n",
    "\n",
    "#### Variables de Particionamiento\n",
    "Se utilizaron las siguientes variables categóricas identificadas en el análisis previo:\n",
    "\n",
    "1. **recommended:** Si el usuario recomienda el juego (True/False).\n",
    "\n",
    "2. **received_for_free:** Si el juego fue recibido gratis (True/False).\n",
    "\n",
    "3. **language:** Idioma de la reseña (ej. english, spanish, russian)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Cálculo de Fracciones de Muestreo por Estrato\n",
    "\n",
    "Para construir una muestra `M` representativa y de tamaño controlado (`TAMAÑO_M`):\n",
    "\n",
    "1. **Fracción por estrato**:\n",
    "   - Se calcula como `(TAMAÑO_M * proporción_original) / conteo_original`.\n",
    "   - Esto asegura que cada estrato contribuya a `M` en proporción a su peso en el dataset original.\n",
    "\n",
    "2. **Límite del 100%**:\n",
    "   - Si un estrato es demasiado pequeño para contribuir al tamaño deseado sin superar el 100%, se incluye completo.\n",
    "\n",
    "3. **Diccionario de estratos**:\n",
    "   - Clave: Tupla `(language, received_for_free, recommended)`.\n",
    "   - Valor: Fracción de muestreo (ej: `0.3` para tomar el 30% del estrato)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:23:32.846175Z",
     "start_time": "2025-06-09T02:21:26.867109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calcular conteos y proporciones originales\n",
    "original_counts = df.groupBy(\"language\", \"received_for_free\", \"recommended\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"proportion\", col(\"count\") / df.count())\n",
    "\n",
    "# Mostrar estratos y sus proporciones\n",
    "original_counts.orderBy(\"proportion\", ascending=False).show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "|language|received_for_free|recommended|  count|          proportion|\n",
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "| english|            false|       true|8335990| 0.38331024012051845|\n",
      "|schinese|            false|       true|2832942|  0.1302659526064093|\n",
      "| russian|            false|       true|1998749| 0.09190761494803211|\n",
      "| english|            false|      false|1020063| 0.04690511786459154|\n",
      "|schinese|            false|      false| 844851|0.038848419884867924|\n",
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:25:36.312831Z",
     "start_time": "2025-06-09T02:24:33.944698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, concat_ws\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 1: Definir tamaño objetivo de la muestra M\n",
    "# --------------------------------------------------------\n",
    "TARGET_SAMPLE_SIZE = 2_000_000  # Puedes ajustar este valor según tu caso\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 2: Calcular la fracción de muestreo para cada estrato\n",
    "# (estratificando por: language, received_for_free, recommended)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "strata_df = original_counts.withColumn(\n",
    "    \"fraction\",\n",
    "    (TARGET_SAMPLE_SIZE * col(\"proportion\")) / col(\"count\")\n",
    ").withColumn(\n",
    "    \"fraction\",\n",
    "    col(\"fraction\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"fraction\",\n",
    "    when(col(\"fraction\") > 1.0, 1.0).otherwise(col(\"fraction\"))\n",
    ")\n",
    "\n",
    "strata_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "| language|received_for_free|recommended|  count|          proportion|           fraction|\n",
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "|  turkish|            false|       true| 552688|0.025414014411213198|0.09196513914256578|\n",
      "|  turkish|            false|      false|  59662| 0.00274341206576188| 0.0919651391425658|\n",
      "|  english|            false|      false|1020063| 0.04690511786459154| 0.0919651391425658|\n",
      "|bulgarian|            false|       true|   8723|4.011059543703007E-4| 0.0919651391425658|\n",
      "|    czech|            false|       true| 119043|0.005473903029474229|0.09196513914256578|\n",
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:27:34.032705Z",
     "start_time": "2025-06-09T02:27:33.841463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------\n",
    "# Paso 3: Crear columna clave combinada para sampleBy()\n",
    "# --------------------------------------------------------\n",
    "# Esto nos permite usar sampleBy() incluso si la estratificación\n",
    "# se basa en múltiples columnas.\n",
    "strata_df = strata_df.withColumn(\n",
    "    \"stratum_key\",\n",
    "    concat_ws(\"_\", \"language\", \"received_for_free\", \"recommended\")\n",
    ")\n",
    "\n",
    "# Crear también la columna clave en el DataFrame completo\n",
    "df = df.withColumn(\n",
    "    \"stratum_key\",\n",
    "    concat_ws(\"_\", \"language\", \"received_for_free\", \"recommended\")\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "|_c0|app_id|            app_name|review_id|language|                            review|timestamp_created|timestamp_updated|recommended|votes_helpful|votes_funny|weighted_vote_score|comment_count|steam_purchase|received_for_free|written_during_early_access|   author_steamid|author_num_games_owned|author_num_reviews|author_playtime_forever|author_playtime_last_two_weeks|author_playtime_at_review|author_last_played|        stratum_key|\n",
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "|  0|292030|The Witcher 3: Wi...| 85185598|schinese|不玩此生遗憾，RPG游戏里的天花板...|       1611381629|       1611381629|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199095369542|                     6|                 2|                 1909.0|                        1448.0|                   1909.0|     1.611343383E9|schinese_false_true|\n",
      "|  1|292030|The Witcher 3: Wi...| 85185250|schinese|       拔DIAO无情打桩机--杰洛特!!!|       1611381030|       1611381030|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561198949504115|                    30|                10|                 2764.0|                        2743.0|                   2674.0|     1.611386307E9|schinese_false_true|\n",
      "|  2|292030|The Witcher 3: Wi...| 85185111|schinese|                           巫师3NB|       1611380800|       1611380800|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199090098988|                     5|                 1|                 1061.0|                        1061.0|                   1060.0|     1.611383777E9|schinese_false_true|\n",
      "|  3|292030|The Witcher 3: Wi...| 85184605| english|              One of the best R...|       1611379970|       1611379970|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199054755373|                     5|                 3|                 5587.0|                        3200.0|                   5524.0|     1.611383744E9| english_false_true|\n",
      "|  4|292030|The Witcher 3: Wi...| 85184287|schinese|                              大作|       1611379427|       1611379427|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199028326951|                     7|                 4|                  217.0|                          42.0|                    217.0|     1.610788249E9|schinese_false_true|\n",
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:30:42.105269Z",
     "start_time": "2025-06-09T02:27:34.040721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------\n",
    "# Paso 4: Convertir las fracciones de muestreo a diccionario\n",
    "# --------------------------------------------------------\n",
    "strata_fractions = {\n",
    "    row[\"stratum_key\"]: row[\"fraction\"]\n",
    "    for row in strata_df.select(\"stratum_key\", \"fraction\").collect()\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 5: Aplicar sampleBy() con la columna stratum_key\n",
    "# --------------------------------------------------------\n",
    "# Esto genera automáticamente la muestra estratificada M\n",
    "# sin necesidad de usar múltiples filtros ni uniones.\n",
    "\n",
    "sample_M = df.sampleBy(\n",
    "    \"stratum_key\",\n",
    "    fractions=strata_fractions,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 6 (opcional): Verificar tamaño y distribución de la muestra M\n",
    "# --------------------------------------------------------\n",
    "print(f\"Tamaño total de la muestra M: {sample_M.count()}\")\n",
    "sample_M.groupBy(\"language\", \"received_for_free\", \"recommended\").count().show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño total de la muestra M: 2001342\n",
      "+----------+-----------------+-----------+-----+\n",
      "|  language|received_for_free|recommended|count|\n",
      "+----------+-----------------+-----------+-----+\n",
      "|   turkish|            false|       true|51115|\n",
      "|   english|            false|      false|93911|\n",
      "|     czech|            false|       true|10928|\n",
      "|   russian|             true|       true| 8791|\n",
      "|   turkish|            false|      false| 5435|\n",
      "| brazilian|            false|      false| 4288|\n",
      "|    polish|            false|      false| 2328|\n",
      "|   english|             true|       true|22760|\n",
      "|   spanish|             true|       true| 2395|\n",
      "|   finnish|             true|       true|  150|\n",
      "| bulgarian|            false|       true|  788|\n",
      "| bulgarian|            false|      false|   79|\n",
      "|   swedish|             true|       true|  275|\n",
      "| norwegian|             true|       true|  127|\n",
      "|  japanese|             true|       true|  178|\n",
      "|   spanish|             true|      false|  187|\n",
      "|  romanian|             true|       true|  330|\n",
      "|     dutch|             true|       true|  154|\n",
      "|portuguese|            false|      false|  375|\n",
      "|   swedish|             true|      false|   13|\n",
      "+----------+-----------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Construcción del conjunto de entrenamiento y prueba\n",
    "### 2.1 Limpieza de datos\n",
    "\n",
    "Antes de constuir el conjunto de entrenamiento y prueba es necesario realizar una limpieza de los datos"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:37:41.114507Z",
     "start_time": "2025-06-09T02:36:10.534679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, length, isnan, count\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 1: Eliminar filas con valores nulos en columnas clave\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = sample_M.dropna(subset=[\"recommended\", \"author_playtime_forever\", \"votes_helpful\", \"review\"])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 2: Truncar valores outliers de 'votes_helpful'\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\n",
    "    \"votes_helpful\",\n",
    "    when(col(\"votes_helpful\") > 1000, 1000).otherwise(col(\"votes_helpful\"))\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 3: Convertir variables booleanas a numéricas\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\n",
    "    \"recommended_numeric\",\n",
    "    when(col(\"recommended\") == True, 1).otherwise(0)\n",
    ").drop(\"recommended\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ✅ Paso 4: Crear nueva columna de longitud de la reseña\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\"review_length\", length(col(\"review\")))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 5 (opcional): Validaciones rápidas\n",
    "# --------------------------------------------------------\n",
    "print(\"Validación de datos después del preprocesamiento:\")\n",
    "clean_sample_M.select(\"recommended_numeric\", \"votes_helpful\", \"review_length\").show(5)\n",
    "\n",
    "# Validar nulos en columnas críticas\n",
    "clean_sample_M.select([\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c)\n",
    "    for c in [\"author_playtime_forever\", \"votes_helpful\", \"review_length\"]\n",
    "]).show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación de datos después del preprocesamiento:\n",
      "+-------------------+-------------+-------------+\n",
      "|recommended_numeric|votes_helpful|review_length|\n",
      "+-------------------+-------------+-------------+\n",
      "|                  1|            0|            2|\n",
      "|                  1|            0|           18|\n",
      "|                  1|            0|           59|\n",
      "|                  1|            0|           30|\n",
      "|                  1|            0|           19|\n",
      "+-------------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------+-------------+-------------+\n",
      "|author_playtime_forever|votes_helpful|review_length|\n",
      "+-----------------------+-------------+-------------+\n",
      "|                      0|            0|            0|\n",
      "+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Preparación de la muestra - vectorización de features\n",
    "Antes de entrenar un modelo de Machine Learning con PySpark, debemos convertir las variables predictoras en un vector numérico. Este paso es crucial porque los algoritmos de ML de Spark requieren que todas las variables de entrada estén en una sola columna de tipo Vector.\n",
    "\n",
    "#### ¿Qué es VectorAssembler?\n",
    "Es una herramienta de PySpark que permite:\n",
    "   - Tomar múltiples columnas individuales (números o variables codificadas)\n",
    "   - Agruparlas en una única columna llamada features (tipo Vector)\n",
    "\n",
    "#### Selección de variables\n",
    "**Variable objetivo (target):** Es la que queremos predecir con nuestro modelo.\n",
    "En este caso, es `recommended_numeric`, que indica si un usuario recomendó el producto (1) o no (0).\n",
    "\n",
    "**Variables predictoras (features):** Son las columnas que el modelo usará para hacer esa predicción.\n",
    "   - Cuántas horas jugó el usuario (`author_playtime_forever`)\n",
    "   - Cuántos votos de utilidad recibió la reseña (`votes_helpful`)\n",
    "   - Qué tan larga fue la reseña (`review_length`)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:42:18.052556Z",
     "start_time": "2025-06-09T02:40:14.561157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Variables predictoras\n",
    "predictor_columns = [\n",
    "    \"author_playtime_forever\",\n",
    "    \"votes_helpful\",\n",
    "    \"review_length\",\n",
    "    \"received_for_free\"\n",
    "]\n",
    "\n",
    "# Creamos el VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=predictor_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Aplicamos el transformador al DataFrame limpio\n",
    "vectorized_df = assembler.transform(clean_sample_M).cache() # Cache for faster development\n",
    "vectorized_df.count()  # Triggers caching immediately"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:45:12.265801Z",
     "start_time": "2025-06-09T02:45:12.255046Z"
    }
   },
   "cell_type": "code",
   "source": "vectorized_df.printSchema()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- app_name: string (nullable = true)\n",
      " |-- review_id: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      " |-- timestamp_created: integer (nullable = true)\n",
      " |-- timestamp_updated: long (nullable = true)\n",
      " |-- votes_helpful: long (nullable = true)\n",
      " |-- votes_funny: long (nullable = true)\n",
      " |-- weighted_vote_score: double (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- steam_purchase: boolean (nullable = true)\n",
      " |-- received_for_free: boolean (nullable = true)\n",
      " |-- written_during_early_access: boolean (nullable = true)\n",
      " |-- author_steamid: long (nullable = true)\n",
      " |-- author_num_games_owned: long (nullable = true)\n",
      " |-- author_num_reviews: long (nullable = true)\n",
      " |-- author_playtime_forever: double (nullable = true)\n",
      " |-- author_playtime_last_two_weeks: double (nullable = true)\n",
      " |-- author_playtime_at_review: double (nullable = true)\n",
      " |-- author_last_played: double (nullable = true)\n",
      " |-- stratum_key: string (nullable = false)\n",
      " |-- recommended_numeric: integer (nullable = false)\n",
      " |-- review_length: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Construcción Train – Test\n",
    "**Objetivo:** Dividir el conjunto M (la muestra estratificada) en dos particiones:\n",
    "   * Conjunto de entrenamiento: Tri\n",
    "   * Conjunto de prueba: Tsi\n",
    "\n",
    "Asegurando:\n",
    "   * Que cada estrato Mi conserve su proporción.\n",
    "   * Que no haya intersección entre Tri y Tsi.\n",
    "   * Que su unión forme M: Tri ∪ Tsi = M\n",
    "\n",
    "Nos apoyaremos en la variable `stratum_key` generada previamente para estratificación, la cual representa las combinaciones de variables de caracterización.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:48:01.022336Z",
     "start_time": "2025-06-09T02:47:59.789578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for nulls or blanks\n",
    "vectorized_df.filter(\"stratum_key IS NULL\").count()\n",
    "vectorized_df.filter(\"stratum_key = ''\").count()\n",
    "\n",
    "# Sample only non-null values\n",
    "vectorized_df.filter(\"stratum_key IS NOT NULL\").select(\"stratum_key\").distinct().show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         stratum_key|\n",
      "+--------------------+\n",
      "|   french_false_true|\n",
      "| tchinese_false_true|\n",
      "|  koreana_false_true|\n",
      "|brazilian_false_f...|\n",
      "|  swedish_false_true|\n",
      "|  polish_false_false|\n",
      "|vietnamese_false_...|\n",
      "|   koreana_true_true|\n",
      "|     latam_true_true|\n",
      "|   danish_false_true|\n",
      "|tchinese_false_false|\n",
      "|    german_true_true|\n",
      "|  english_true_false|\n",
      "|japanese_false_false|\n",
      "| italian_false_false|\n",
      "|bulgarian_false_f...|\n",
      "|   swedish_true_true|\n",
      "| norwegian_true_true|\n",
      "|   latam_false_false|\n",
      "| swedish_false_false|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "SEED = 42\n",
    "train_fraction = 0.8  # 80% entrenamiento, 20% prueba\n",
    "\n",
    "# 1️⃣ Estratificación: obtenemos todos los valores únicos del estrato\n",
    "stratum_values = vectorized_df.select(\"stratum_key\").distinct().collect()\n",
    "\n",
    "# 2️⃣ Inicializamos DataFrames vacíos para acumulación\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "# 3️⃣ Para cada estrato Mi, dividimos usando randomSplit para evitar sesgos\n",
    "for stratum in stratum_values:\n",
    "    stratum_df = vectorized_df.filter(col(\"stratum_key\") == stratum)\n",
    "    train_df, test_df = stratum_df.randomSplit([train_fraction, 1 - train_fraction], seed=SEED)\n",
    "    train_parts.append(train_df)\n",
    "    test_parts.append(test_df)\n",
    "\n",
    "# 4️⃣ Unimos todos los conjuntos de entrenamiento y prueba\n",
    "train_set = train_parts[0]\n",
    "for df in train_parts[1:]:\n",
    "    train_set = train_set.union(df)\n",
    "\n",
    "test_set = test_parts[0]\n",
    "for df in test_parts[1:]:\n",
    "    test_set = test_set.union(df)\n",
    "\n",
    "# 5️⃣ Verificación de integridad\n",
    "print(\"Entrenamiento:\", train_set.count())\n",
    "print(\"Prueba:\", test_set.count())\n",
    "print(\"Total:\", vectorized_df.count())\n",
    "print(\"¿Intersección vacía?\", train_set.intersect(test_set).count() == 0)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
