{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsZKKvxwYF3o"
   },
   "source": [
    "# Actividad 4 | Métricas de calidad de resultados\n",
    "Identificar métricas para la medición de la calidad de resultados derivados de la aplicación de modelos de aprendizaje, ya sea supervisado o no supervisado, orientado al procesamiento de grandes volúmenes de datos, que permitan la selección de los modelos que mejor se ajusten a la tarea de aprendizaje a resolver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjmB97B-X-hU"
   },
   "source": "## Alejandro González Almazán - A00517113\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0n2G2l_Yaze"
   },
   "source": "--------------------------------------------------------------------------------"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar8PqEFjYfVA"
   },
   "source": "# Impotacion de Librerias"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rFrmYAiPYdd9",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:02.569411Z",
     "start_time": "2025-06-09T02:20:02.133065Z"
    }
   },
   "source": [
    "# PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\"\"\"\n",
    "# omitir para ejecutar de forma local\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\"\"\"\n",
    "#Librerias de codigo\n",
    "import kagglehub\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXLHPUosZE5_"
   },
   "source": "### Inicializar entorno PySpark"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "GCAP0o7kcogx",
    "outputId": "1ed9ae58-97d4-422d-b102-c39be4f404db",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:12.357232Z",
     "start_time": "2025-06-09T02:20:02.652399Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analisis_Steam\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b569715be0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Alex:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Analisis_Steam</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm2nuBbCZct4"
   },
   "source": "### Lectura de datos"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nTiLMXpKZ3z9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "750a6611-b09f-4fb9-91a3-edf92c244175",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.376742Z",
     "start_time": "2025-06-09T02:20:13.139045Z"
    }
   },
   "source": "file_path = kagglehub.dataset_download(\"najzeko/steam-reviews-2021\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.632899Z",
     "start_time": "2025-06-09T02:20:13.629182Z"
    }
   },
   "cell_type": "code",
   "source": "print(file_path)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\najzeko\\steam-reviews-2021\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlaqOdpzrRV5",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:20:13.719225Z",
     "start_time": "2025-06-09T02:20:13.713104Z"
    }
   },
   "source": "print(f\"{file_path}/steam_reviews.csv\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\.cache\\kagglehub\\datasets\\najzeko\\steam-reviews-2021\\versions\\1/steam_reviews.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BGv7gw7jZdgb",
    "ExecuteTime": {
     "end_time": "2025-06-09T02:21:24.728261Z",
     "start_time": "2025-06-09T02:20:13.814533Z"
    }
   },
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"sep\", \",\").option(\"escape\", \"\\\"\").csv(f\"{file_path}/steam_reviews.csv\")\n",
    "\n",
    "new_columns = [col_name.replace(\".\", \"_\") for col_name in df.columns]\n",
    "df = df.toDF(*new_columns)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F36rcs_erRWC"
   },
   "source": [
    "## 1. Construcción de la muestra M (Particiones Mi) con PySpark\n",
    "Para construir una muestra M representativa de la población P (dataset de reseñas de Steam), se generarán particiones Mi basadas en variables de caracterización clave. El objetivo es asegurar que cada Mi mantenga proporcionalidad respecto a la distribución original, evitando sesgos.\n",
    "\n",
    "#### Variables de Particionamiento\n",
    "Se utilizaron las siguientes variables categóricas identificadas en el análisis previo:\n",
    "\n",
    "1. **recommended:** Si el usuario recomienda el juego (True/False).\n",
    "\n",
    "2. **received_for_free:** Si el juego fue recibido gratis (True/False).\n",
    "\n",
    "3. **language:** Idioma de la reseña (ej. english, spanish, russian)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Cálculo de Fracciones de Muestreo por Estrato\n",
    "\n",
    "Para construir una muestra `M` representativa y de tamaño controlado (`TAMAÑO_M`):\n",
    "\n",
    "1. **Fracción por estrato**:\n",
    "   - Se calcula como `(TAMAÑO_M * proporción_original) / conteo_original`.\n",
    "   - Esto asegura que cada estrato contribuya a `M` en proporción a su peso en el dataset original.\n",
    "\n",
    "2. **Límite del 100%**:\n",
    "   - Si un estrato es demasiado pequeño para contribuir al tamaño deseado sin superar el 100%, se incluye completo.\n",
    "\n",
    "3. **Diccionario de estratos**:\n",
    "   - Clave: Tupla `(language, received_for_free, recommended)`.\n",
    "   - Valor: Fracción de muestreo (ej: `0.3` para tomar el 30% del estrato)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:23:32.846175Z",
     "start_time": "2025-06-09T02:21:26.867109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calcular conteos y proporciones originales\n",
    "original_counts = df.groupBy(\"language\", \"received_for_free\", \"recommended\") \\\n",
    "    .count() \\\n",
    "    .withColumn(\"proportion\", col(\"count\") / df.count())\n",
    "\n",
    "# Mostrar estratos y sus proporciones\n",
    "original_counts.orderBy(\"proportion\", ascending=False).show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "|language|received_for_free|recommended|  count|          proportion|\n",
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "| english|            false|       true|8335990| 0.38331024012051845|\n",
      "|schinese|            false|       true|2832942|  0.1302659526064093|\n",
      "| russian|            false|       true|1998749| 0.09190761494803211|\n",
      "| english|            false|      false|1020063| 0.04690511786459154|\n",
      "|schinese|            false|      false| 844851|0.038848419884867924|\n",
      "+--------+-----------------+-----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:25:36.312831Z",
     "start_time": "2025-06-09T02:24:33.944698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, concat_ws\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 1: Definir tamaño objetivo de la muestra M\n",
    "# --------------------------------------------------------\n",
    "TARGET_SAMPLE_SIZE = 2_000_000  # Puedes ajustar este valor según tu caso\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 2: Calcular la fracción de muestreo para cada estrato\n",
    "# (estratificando por: language, received_for_free, recommended)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "strata_df = original_counts.withColumn(\n",
    "    \"fraction\",\n",
    "    (TARGET_SAMPLE_SIZE * col(\"proportion\")) / col(\"count\")\n",
    ").withColumn(\n",
    "    \"fraction\",\n",
    "    col(\"fraction\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"fraction\",\n",
    "    when(col(\"fraction\") > 1.0, 1.0).otherwise(col(\"fraction\"))\n",
    ")\n",
    "\n",
    "strata_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "| language|received_for_free|recommended|  count|          proportion|           fraction|\n",
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "|  turkish|            false|       true| 552688|0.025414014411213198|0.09196513914256578|\n",
      "|  turkish|            false|      false|  59662| 0.00274341206576188| 0.0919651391425658|\n",
      "|  english|            false|      false|1020063| 0.04690511786459154| 0.0919651391425658|\n",
      "|bulgarian|            false|       true|   8723|4.011059543703007E-4| 0.0919651391425658|\n",
      "|    czech|            false|       true| 119043|0.005473903029474229|0.09196513914256578|\n",
      "+---------+-----------------+-----------+-------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:27:34.032705Z",
     "start_time": "2025-06-09T02:27:33.841463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------\n",
    "# Paso 3: Crear columna clave combinada para sampleBy()\n",
    "# --------------------------------------------------------\n",
    "# Esto nos permite usar sampleBy() incluso si la estratificación\n",
    "# se basa en múltiples columnas.\n",
    "strata_df = strata_df.withColumn(\n",
    "    \"stratum_key\",\n",
    "    concat_ws(\"_\", \"language\", \"received_for_free\", \"recommended\")\n",
    ")\n",
    "\n",
    "# Crear también la columna clave en el DataFrame completo\n",
    "df = df.withColumn(\n",
    "    \"stratum_key\",\n",
    "    concat_ws(\"_\", \"language\", \"received_for_free\", \"recommended\")\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "|_c0|app_id|            app_name|review_id|language|                            review|timestamp_created|timestamp_updated|recommended|votes_helpful|votes_funny|weighted_vote_score|comment_count|steam_purchase|received_for_free|written_during_early_access|   author_steamid|author_num_games_owned|author_num_reviews|author_playtime_forever|author_playtime_last_two_weeks|author_playtime_at_review|author_last_played|        stratum_key|\n",
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "|  0|292030|The Witcher 3: Wi...| 85185598|schinese|不玩此生遗憾，RPG游戏里的天花板...|       1611381629|       1611381629|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199095369542|                     6|                 2|                 1909.0|                        1448.0|                   1909.0|     1.611343383E9|schinese_false_true|\n",
      "|  1|292030|The Witcher 3: Wi...| 85185250|schinese|       拔DIAO无情打桩机--杰洛特!!!|       1611381030|       1611381030|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561198949504115|                    30|                10|                 2764.0|                        2743.0|                   2674.0|     1.611386307E9|schinese_false_true|\n",
      "|  2|292030|The Witcher 3: Wi...| 85185111|schinese|                           巫师3NB|       1611380800|       1611380800|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199090098988|                     5|                 1|                 1061.0|                        1061.0|                   1060.0|     1.611383777E9|schinese_false_true|\n",
      "|  3|292030|The Witcher 3: Wi...| 85184605| english|              One of the best R...|       1611379970|       1611379970|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199054755373|                     5|                 3|                 5587.0|                        3200.0|                   5524.0|     1.611383744E9| english_false_true|\n",
      "|  4|292030|The Witcher 3: Wi...| 85184287|schinese|                              大作|       1611379427|       1611379427|       true|            0|          0|                0.0|            0|          true|            false|                      false|76561199028326951|                     7|                 4|                  217.0|                          42.0|                    217.0|     1.610788249E9|schinese_false_true|\n",
      "+---+------+--------------------+---------+--------+----------------------------------+-----------------+-----------------+-----------+-------------+-----------+-------------------+-------------+--------------+-----------------+---------------------------+-----------------+----------------------+------------------+-----------------------+------------------------------+-------------------------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:30:42.105269Z",
     "start_time": "2025-06-09T02:27:34.040721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------\n",
    "# Paso 4: Convertir las fracciones de muestreo a diccionario\n",
    "# --------------------------------------------------------\n",
    "strata_fractions = {\n",
    "    row[\"stratum_key\"]: row[\"fraction\"]\n",
    "    for row in strata_df.select(\"stratum_key\", \"fraction\").collect()\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 5: Aplicar sampleBy() con la columna stratum_key\n",
    "# --------------------------------------------------------\n",
    "# Esto genera automáticamente la muestra estratificada M\n",
    "# sin necesidad de usar múltiples filtros ni uniones.\n",
    "\n",
    "sample_M = df.sampleBy(\n",
    "    \"stratum_key\",\n",
    "    fractions=strata_fractions,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 6 (opcional): Verificar tamaño y distribución de la muestra M\n",
    "# --------------------------------------------------------\n",
    "print(f\"Tamaño total de la muestra M: {sample_M.count()}\")\n",
    "sample_M.groupBy(\"language\", \"received_for_free\", \"recommended\").count().show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño total de la muestra M: 2001342\n",
      "+----------+-----------------+-----------+-----+\n",
      "|  language|received_for_free|recommended|count|\n",
      "+----------+-----------------+-----------+-----+\n",
      "|   turkish|            false|       true|51115|\n",
      "|   english|            false|      false|93911|\n",
      "|     czech|            false|       true|10928|\n",
      "|   russian|             true|       true| 8791|\n",
      "|   turkish|            false|      false| 5435|\n",
      "| brazilian|            false|      false| 4288|\n",
      "|    polish|            false|      false| 2328|\n",
      "|   english|             true|       true|22760|\n",
      "|   spanish|             true|       true| 2395|\n",
      "|   finnish|             true|       true|  150|\n",
      "| bulgarian|            false|       true|  788|\n",
      "| bulgarian|            false|      false|   79|\n",
      "|   swedish|             true|       true|  275|\n",
      "| norwegian|             true|       true|  127|\n",
      "|  japanese|             true|       true|  178|\n",
      "|   spanish|             true|      false|  187|\n",
      "|  romanian|             true|       true|  330|\n",
      "|     dutch|             true|       true|  154|\n",
      "|portuguese|            false|      false|  375|\n",
      "|   swedish|             true|      false|   13|\n",
      "+----------+-----------------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Construcción del conjunto de entrenamiento y prueba\n",
    "### 2.1 Limpieza de datos\n",
    "\n",
    "Antes de constuir el conjunto de entrenamiento y prueba es necesario realizar una limpieza de los datos"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:37:41.114507Z",
     "start_time": "2025-06-09T02:36:10.534679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, when, length, isnan, count\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 1: Eliminar filas con valores nulos en columnas clave\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = sample_M.dropna(subset=[\"recommended\", \"author_playtime_forever\", \"votes_helpful\", \"review\"])\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 2: Truncar valores outliers de 'votes_helpful'\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\n",
    "    \"votes_helpful\",\n",
    "    when(col(\"votes_helpful\") > 1000, 1000).otherwise(col(\"votes_helpful\"))\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 3: Convertir variables booleanas a numéricas\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\n",
    "    \"recommended_numeric\",\n",
    "    when(col(\"recommended\") == True, 1).otherwise(0)\n",
    ").drop(\"recommended\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# ✅ Paso 4: Crear nueva columna de longitud de la reseña\n",
    "# --------------------------------------------------------\n",
    "clean_sample_M = clean_sample_M.withColumn(\"review_length\", length(col(\"review\")))\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Paso 5 (opcional): Validaciones rápidas\n",
    "# --------------------------------------------------------\n",
    "print(\"Validación de datos después del preprocesamiento:\")\n",
    "clean_sample_M.select(\"recommended_numeric\", \"votes_helpful\", \"review_length\").show(5)\n",
    "\n",
    "# Validar nulos en columnas críticas\n",
    "clean_sample_M.select([\n",
    "    count(when(col(c).isNull() | isnan(c), c)).alias(c)\n",
    "    for c in [\"author_playtime_forever\", \"votes_helpful\", \"review_length\"]\n",
    "]).show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación de datos después del preprocesamiento:\n",
      "+-------------------+-------------+-------------+\n",
      "|recommended_numeric|votes_helpful|review_length|\n",
      "+-------------------+-------------+-------------+\n",
      "|                  1|            0|            2|\n",
      "|                  1|            0|           18|\n",
      "|                  1|            0|           59|\n",
      "|                  1|            0|           30|\n",
      "|                  1|            0|           19|\n",
      "+-------------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------+-------------+-------------+\n",
      "|author_playtime_forever|votes_helpful|review_length|\n",
      "+-----------------------+-------------+-------------+\n",
      "|                      0|            0|            0|\n",
      "+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Preparación de la muestra - vectorización de features\n",
    "Antes de entrenar un modelo de Machine Learning con PySpark, debemos convertir las variables predictoras en un vector numérico. Este paso es crucial porque los algoritmos de ML de Spark requieren que todas las variables de entrada estén en una sola columna de tipo Vector.\n",
    "\n",
    "#### ¿Qué es VectorAssembler?\n",
    "Es una herramienta de PySpark que permite:\n",
    "   - Tomar múltiples columnas individuales (números o variables codificadas)\n",
    "   - Agruparlas en una única columna llamada features (tipo Vector)\n",
    "\n",
    "#### Selección de variables\n",
    "**Variable objetivo (target):** Es la que queremos predecir con nuestro modelo.\n",
    "En este caso, es `recommended_numeric`, que indica si un usuario recomendó el producto (1) o no (0).\n",
    "\n",
    "**Variables predictoras (features):** Son las columnas que el modelo usará para hacer esa predicción.\n",
    "   - Cuántas horas jugó el usuario (`author_playtime_forever`)\n",
    "   - Cuántos votos de utilidad recibió la reseña (`votes_helpful`)\n",
    "   - Qué tan larga fue la reseña (`review_length`)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:42:18.052556Z",
     "start_time": "2025-06-09T02:40:14.561157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Variables predictoras\n",
    "predictor_columns = [\n",
    "    \"author_playtime_forever\",\n",
    "    \"votes_helpful\",\n",
    "    \"review_length\",\n",
    "    \"received_for_free\"\n",
    "]\n",
    "\n",
    "# Creamos el VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=predictor_columns,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Aplicamos el transformador al DataFrame limpio\n",
    "vectorized_df = assembler.transform(clean_sample_M).cache() # Cache for faster development\n",
    "vectorized_df.count()  # Triggers caching immediately"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1998216"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:45:12.265801Z",
     "start_time": "2025-06-09T02:45:12.255046Z"
    }
   },
   "cell_type": "code",
   "source": "vectorized_df.printSchema()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- app_name: string (nullable = true)\n",
      " |-- review_id: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- review: string (nullable = true)\n",
      " |-- timestamp_created: integer (nullable = true)\n",
      " |-- timestamp_updated: long (nullable = true)\n",
      " |-- votes_helpful: long (nullable = true)\n",
      " |-- votes_funny: long (nullable = true)\n",
      " |-- weighted_vote_score: double (nullable = true)\n",
      " |-- comment_count: integer (nullable = true)\n",
      " |-- steam_purchase: boolean (nullable = true)\n",
      " |-- received_for_free: boolean (nullable = true)\n",
      " |-- written_during_early_access: boolean (nullable = true)\n",
      " |-- author_steamid: long (nullable = true)\n",
      " |-- author_num_games_owned: long (nullable = true)\n",
      " |-- author_num_reviews: long (nullable = true)\n",
      " |-- author_playtime_forever: double (nullable = true)\n",
      " |-- author_playtime_last_two_weeks: double (nullable = true)\n",
      " |-- author_playtime_at_review: double (nullable = true)\n",
      " |-- author_last_played: double (nullable = true)\n",
      " |-- stratum_key: string (nullable = false)\n",
      " |-- recommended_numeric: integer (nullable = false)\n",
      " |-- review_length: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Construcción Train – Test\n",
    "**Objetivo:** Dividir el conjunto M (la muestra estratificada) en dos particiones:\n",
    "   * Conjunto de entrenamiento: Tri\n",
    "   * Conjunto de prueba: Tsi\n",
    "\n",
    "Asegurando:\n",
    "   * Que cada estrato Mi conserve su proporción.\n",
    "   * Que no haya intersección entre Tri y Tsi.\n",
    "   * Que su unión forme M: Tri ∪ Tsi = M\n",
    "\n",
    "Nos apoyaremos en la variable `stratum_key` generada previamente para estratificación, la cual representa las combinaciones de variables de caracterización.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:48:01.022336Z",
     "start_time": "2025-06-09T02:47:59.789578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for nulls or blanks\n",
    "vectorized_df.filter(\"stratum_key IS NULL\").count()\n",
    "vectorized_df.filter(\"stratum_key = ''\").count()\n",
    "\n",
    "# Sample only non-null values\n",
    "vectorized_df.filter(\"stratum_key IS NOT NULL\").select(\"stratum_key\").distinct().show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         stratum_key|\n",
      "+--------------------+\n",
      "|   french_false_true|\n",
      "| tchinese_false_true|\n",
      "|  koreana_false_true|\n",
      "|brazilian_false_f...|\n",
      "|  swedish_false_true|\n",
      "|  polish_false_false|\n",
      "|vietnamese_false_...|\n",
      "|   koreana_true_true|\n",
      "|     latam_true_true|\n",
      "|   danish_false_true|\n",
      "|tchinese_false_false|\n",
      "|    german_true_true|\n",
      "|  english_true_false|\n",
      "|japanese_false_false|\n",
      "| italian_false_false|\n",
      "|bulgarian_false_f...|\n",
      "|   swedish_true_true|\n",
      "| norwegian_true_true|\n",
      "|   latam_false_false|\n",
      "| swedish_false_false|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T02:59:42.331644Z",
     "start_time": "2025-06-09T02:58:53.596831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "SEED = 42\n",
    "train_fraction = 0.8\n",
    "\n",
    "stratum_values = vectorized_df.select(\"stratum_key\").distinct().collect()\n",
    "\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for stratum in stratum_values:\n",
    "    stratum_value = stratum['stratum_key']  # Extract the string value here\n",
    "    stratum_df = vectorized_df.filter(col(\"stratum_key\") == stratum_value)\n",
    "    train_df, test_df = stratum_df.randomSplit([train_fraction, 1 - train_fraction], seed=SEED)\n",
    "    train_parts.append(train_df)\n",
    "    test_parts.append(test_df)\n",
    "\n",
    "train_set = train_parts[0]\n",
    "for df in train_parts[1:]:\n",
    "    train_set = train_set.union(df)\n",
    "\n",
    "test_set = test_parts[0]\n",
    "for df in test_parts[1:]:\n",
    "    test_set = test_set.union(df)\n",
    "\n",
    "print(\"Entrenamiento:\", train_set.count())\n",
    "print(\"Prueba:\", test_set.count())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 1600313\n",
      "Prueba: 397903\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Selección de métricas para medir calidad de resultados\n",
    "\n",
    "Para evaluar la calidad de los modelos entrenados en este proyecto, se han seleccionado métricas que permiten medir el desempeño en problemas de clasificación binaria, considerando el volumen grande de datos con el que se trabaja.\n",
    "\n",
    "Dado que la variable objetivo es `recommended_numeric`, que indica si un usuario recomendó el producto (1) o no (0), las métricas seleccionadas son:\n",
    "\n",
    "- **Accuracy**: Proporción de predicciones correctas sobre el total. Es útil para tener una idea general del desempeño.\n",
    "- **Precision**: Indica la proporción de verdaderos positivos sobre el total de positivos predichos. Es importante para evitar falsos positivos.\n",
    "- **Recall (Sensibilidad)**: Proporción de verdaderos positivos sobre el total de positivos reales. Fundamental para detectar correctamente recomendaciones.\n",
    "- **F1-score**: Media armónica entre precision y recall, útil cuando hay desequilibrio en las clases.\n",
    "- **Area Under ROC Curve (AUC-ROC)**: Mide la capacidad del modelo para discriminar entre clases a diferentes umbrales, especialmente relevante en conjuntos con gran cantidad de datos y posibles desbalances.\n",
    "\n",
    "\n",
    "Estas métricas se implementarán en la etapa de experimentación para obtener un análisis completo y robusto del comportamiento del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Entrenamiento de Modelos de Aprendizaje\n",
    "\n",
    "A continuación, se describe el proceso de entrenamiento del modelo de clasificación supervisada utilizando PySpark ML.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T03:16:52.319055Z",
     "start_time": "2025-06-09T03:05:10.730542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Definir el modelo\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"recommended_numeric\", maxIter=10)\n",
    "\n",
    "# Definir la grilla de parámetros para optimización\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.x, [0.01, 0.1]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Definir el evaluador\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"recommended_numeric\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Definir el CrossValidator para evitar sobreajuste\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Entrenar el modelo con validación cruzada\n",
    "cv_model = crossval.fit(train_set)\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = cv_model.bestModel"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T03:19:11.244926Z",
     "start_time": "2025-06-09T03:18:16.392913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predecir en el conjunto de prueba\n",
    "predictions = best_model.transform(test_set)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluadores para diferentes métricas\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"recommended_numeric\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"recommended_numeric\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"recommended_numeric\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"recommended_numeric\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "auc = evaluator.evaluate(predictions)  # AUC-ROC evaluador definido arriba\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {g:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8756\n",
      "Precision: 0.7668\n",
      "Recall: 0.8756\n",
      "F1-Score: 0.8176\n",
      "AUC-ROC: 0.6038\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Resultados del Modelo de Clasificación - Recomendaciones de Reseñas\n",
    "\n",
    "#### Métricas de Evaluación\n",
    "\n",
    "| Métrica     | Valor    |\n",
    "|-------------|----------|\n",
    "| Accuracy    | 0.8756   |\n",
    "| Precision   | 0.7668   |\n",
    "| Recall      | 0.8756   |\n",
    "| F1-Score    | 0.8176   |\n",
    "| AUC-ROC     | 0.6038   |\n",
    "\n",
    "### Conclusiones\n",
    "\n",
    "- **Precisión general (Accuracy)** del 87.56% indica que el modelo clasifica correctamente una alta proporción de los ejemplos. Sin embargo, esta métrica por sí sola puede ser engañosa si los datos están desbalanceados.\n",
    "\n",
    "- **Recall (Sensibilidad)** de 87.56% significa que el modelo es bastante efectivo identificando los casos positivos reales (usuarios que recomendaron el producto). Es una señal positiva en términos de cobertura.\n",
    "\n",
    "- **Precisión (Precision)** de 76.68% implica que, de las veces que el modelo predijo una recomendación positiva, en aproximadamente el 77% de los casos tuvo razón. Aún hay margen de mejora para evitar falsos positivos.\n",
    "\n",
    "- **F1-Score** de 81.76% representa un equilibrio entre precisión y recall. Es un buen indicador general del rendimiento del modelo.\n",
    "\n",
    "- **AUC-ROC** de 0.6038 sugiere una capacidad discriminativa limitada del modelo, es decir, no distingue muy bien entre clases positivas y negativas. Un valor ideal estaría más cerca de 1.0. Esta métrica nos alerta que el modelo, si bien acierta muchas predicciones, podría estar tomando decisiones con poca certeza cuando las clases están más mezcladas.\n",
    "\n",
    "#### Recomendaciones\n",
    "\n",
    "- Se puede explorar el uso de algoritmos más complejos o con mayor capacidad de generalización, como Random Forest o Gradient-Boosted Trees, que suelen mejorar la AUC.\n",
    "\n",
    "- Es recomendable analizar más profundamente la distribución de clases, balancear los datos si hay desbalance, y ajustar hiperparámetros para mejorar la capacidad del modelo.\n",
    "\n",
    "- Incorporar más variables (features) o transformar mejor las existentes podría ayudar a capturar patrones más útiles para el modelo.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
